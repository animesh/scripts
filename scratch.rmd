
```{r gene-set enrichment with regularized regression}
#https://usethis.r-lib.org/articles/articles/usethis-setup.html
#usethis::edit_r_environ()
#Sys.getenv("GITHUB_PAT")
#https://github.com/TaoDFang/gerr
#library(devtools)
#install_github("TaoDFang/gerr")
library("gerr")
#https://yulab-smu.github.io/clusterProfiler-book/chapter3.html#input-data
#BiocManager::install("clusterProfiler")
#BiocManager::install("GSEABase")
library(clusterProfiler)
#http://data.wikipathways.org/current/gmt/
wp2gene <- read.gmt("L:/promec/Animesh/Lisa/wikipathways-20191010-gmt-Homo_sapiens.gmt")
library(magrittr)
wp2gene <- wp2gene %>% tidyr::separate(ont, c("name","version","wpid","org"), "%")
wpid2gene <- wp2gene %>% dplyr::select(wpid, gene) #TERM2GENE
wpid2name <- wp2gene %>% dplyr::select(wpid, name) #TERM2NAME
gene=c("4312",  "8318"  ,"10874", "55143" ,"55388" ,"991")
ewp <- enricher(gene, TERM2GENE = wpid2gene, TERM2NAME = wpid2name)
head(ewp)
ewp2 <- GSEA(gene, TERM2GENE = wpid2gene, TERM2NAME = wpid2name, verbose=FALSE)
head(ewp2)
install.packages("msigdbr")
library("msigdbr")
msigdbr_show_species()
m_df = msigdbr(species = "Homo sapiens")
head(m_df)
#https://bioconductor.org/packages/release/data/annotation/html/org.Hs.eg.db.html
BiocManager::install("org.Hs.eg.db")
library(org.Hs.eg.db)
select(org.Hs.eg.db, Rkeys(org.Hs.egUNIPROT)[1:5], "ENTREZID", "UNIPROT")
select(org.Hs.eg.db, gene, "UNIPROT", "ENTREZID")
```

```{r kernel, echo = FALSE}
#https://bookdown.org/yihui/rmarkdown/
setRepositories(graphics = getOption("menu.graphics"),ind = NULL, addURLs = character())
#install.packages("devtools")
#install.packages('IRkernel')
IRkernel::installspec()
#https://rmarkdown.rstudio.com/authoring_shiny.html
```

```{r pClean}
#https://github.com/AimeeD90/pClean_release
install.packages('digest', repos='http://cran.us.r-project.org')
devtools::install_github("AimeeD90/pClean_release")
#remotes::install_github("AimeeD90/pClean_release")
library(pClean)
mgffile<-system.file("extdata/", "tte.frac1.mgf",package="pClean")
pCleanGear(mgf=mgffile,outdir="pClean",mem=2,cpu=0,mionFilter=TRUE,labelMethod="iTRAQ8plex",repFilter=TRUE,labelFilter=TRUE,low=TRUE,high=TRUE,isoReduction=TRUE,chargeDeconv=TRUE,largerThanPrecursor=TRUE,ionsMerge=TRUE,network=TRUE,debug=FALSE)
mgffile<-system.file("extdata/", "120426_Jurkat_highLC_Frac1.mgf",package="pClean")

pCleanGear(mgf=mgffile,outdir="jurkat/result",mem=2,cpu=0,mionFilter=TRUE,isoReduction=TRUE,chargeDeconv=TRUE,largerThanPrecursor=TRUE,ionsMerge=TRUE,network=TRUE,debug=FALSE)

```

```{r imputeMQstyle}
## https://datascienceplus.com/proteomics-data-analysis-2-3-data-filtering-and-missing-value-imputation/ Data imputation function
impute_data = function(df, width = 0.3, downshift = 1.8) {
  # df = data frame containing filtered 
  # Assumes missing data (in df) follows a narrowed and downshifted normal distribution

  LOG2.names = grep("^LOG2", names(df), value = TRUE)
  impute.names = sub("^LOG2", "impute", LOG2.names)

  # Create new columns indicating whether the values are imputed
  df[impute.names] = lapply(LOG2.names, function(x) !is.finite(df[, x]))

  # Imputation
  set.seed(1)
  df[LOG2.names] = lapply(LOG2.names,
                          function(x) {
                            temp = df[[x]]
                            temp[!is.finite(temp)] = NA

                            temp.sd = width * sd(temp[df$KEEP], na.rm = TRUE)   # shrink sd width
                            temp.mean = mean(temp[df$KEEP], na.rm = TRUE) - 
                              downshift * sd(temp[df$KEEP], na.rm = TRUE)   # shift mean of imputed values

                            n.missing = sum(is.na(temp))
                            temp[is.na(temp)] = rnorm(n.missing, mean = temp.mean, sd = temp.sd)                          
                            return(temp)
                          })
  return(df)
}


## Apply imputation
df.FNI = impute_data(df.FN)
```

```{r ggfortify}
install.packages('ggfortify')
library('ggfortify')
autoplot(as.matrix(randu))
```

```{r lme4}
install.packages('gganimate')
library('lme4')
autoplot(as.matrix(randu))
```

```{r multivariance}
install.packages('multivariance')
library('multivariance')
autoplot(as.matrix(randu))
```


```{r ggpubr}
usethis::browse_github_pat()
devtools::install_github("kassambara/ggpubr")
```

```{r compMS}
#https://link.springer.com/chapter/10.1007/978-3-030-05318-5_9
#https://github.com/automl/auto-sklearn via reticulate
#http://www.deeplearningbook.org/contents/monte_carlo.html
#https://github.com/ColtoCaro/compositionalMS/issues/1
devtools::install_github('coltocaro/compMS')
#https://willfondrie.com/2019/02/an-intuitive-look-at-the-xcorr-score-function-in-proteomics/
library(compMS)
RES <- compBayes(sampleDat)

View(RES[[1]])
View(RES[[2]])
View(RES[[4]])
View(RES[[5]])
View(RES[[6]])

catterPlot(RES, byCond = TRUE, avgCond = TRUE)

precisionPlot(RES, avgCond = TRUE)

```

```{r umapr}
#https://github.com/ropenscilabs/umapr#basic-use
devtools::install_github("ropenscilabs/umapr")
library(umapr)
library(tidyverse)

# select only numeric columns
df <- as.matrix(iris[ , 1:4])

# run UMAP algorithm
embedding <- umap(df)
embedding %>% 
  mutate(Species = iris$Species) %>%
  ggplot(aes(UMAP1, UMAP2, color = Species)) + geom_point()
run_umap_shiny(embedding)
```

```{r tfp-install}
#https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/
install.packages("devtools",INSTALL_opts = "--no-multiarch")#https://stackoverflow.com/questions/40644971/error-installing-reporters-and-reportersjars
install.packages("reticulate",INSTALL_opts = "--no-multiarch")
devtools::install_github("rstudio/tfprobability",INSTALL_opts = "--no-multiarch")
```

```{r tfp-data}
library(tensorflow)
library(tfprobability)
#install.packages("tidyverse")
library(tidyverse)
library(zeallot)
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)

#https://raw.githubusercontent.com/rstudio/tensorflow-blog/master/docs/posts/2019-06-25-dynamic_linear_models_tfprobability/data/capm.txt
df <- read_table("capm.txt",col_types = list(X1 = col_date(format = "%Y.%m"))) %>%rename(month = X1)
df %>% glimpse()
df %>% gather(key = "symbol", value = "return", -month) %>%
  ggplot(aes(x = month, y = return, color = symbol)) +
  geom_line() +
  facet_grid(rows = vars(symbol), scales = "free")
```

```{r tfp-explore}
ibm <- df$IBM - df$RKFREE
# market excess returns
x <- df$MARKET - df$RKFREE

fit <- lm(ibm ~ x)
summary(fit)
ts <- ibm %>% matrix()
# forecast 12 months
n_forecast_steps <- 12
ts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]

# make sure we work with float32 here
ts_train <- tf$cast(ts_train, tf$float32)
ts <- tf$cast(ts, tf$float32)
linreg <- ts %>%
  sts_dynamic_linear_regression(
    design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32)
  )
optimizer <- tf$compat$v1$train$AdamOptimizer(0.1)
model <- ts %>%
  sts_dynamic_linear_regression(design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32))

# only train on the training set!    
loss_and_dists <- ts_train %>% sts_build_factored_variational_loss(model = model)
variational_loss <- loss_and_dists[[1]]

train_op <- optimizer$minimize(variational_loss)


fit_vi <-
  function(ts,
           ts_train,
           model,
           n_iterations,
           n_param_samples,
           n_forecast_steps,
           n_forecast_samples) {
    loss_and_dists <-
      ts_train %>% sts_build_factored_variational_loss(model = model)
    variational_loss <- loss_and_dists[[1]]
    train_op <- optimizer$minimize(variational_loss)
    
    with (tf$Session() %as% sess,  {
      sess$run(tf$compat$v1$global_variables_initializer())
      for (step in 1:n_iterations) {
        res <- sess$run(train_op)
        loss <- sess$run(variational_loss)
        if (step %% 1 == 0)
          cat("Loss: ", as.numeric(loss), "\n")
      }
      variational_distributions <- loss_and_dists[[2]]
      posterior_samples <-
        Map(
          function(d)
            d %>% tfd_sample(n_param_samples),
          variational_distributions %>% reticulate::py_to_r() %>% unname()
        )
      forecast_dists <-
        ts_train %>% sts_forecast(model, posterior_samples, n_forecast_steps)
      fc_means <- forecast_dists %>% tfd_mean()
      fc_sds <- forecast_dists %>% tfd_stddev()
      
      c(posterior_samples,
        fc_means,
        fc_sds) %<-%
        sess$run(list(posterior_samples,
                      fc_means,
                      fc_sds))
    })
    
    list(variational_distributions,
         posterior_samples,
         fc_means[, 1],
         fc_sds[, 1])
  }


# call fit_vi defined above
# number of VI steps
n_iterations <- 300
# sample size for posterior samples
n_param_samples <- 50
# sample size to draw from the forecast distribution
n_forecast_samples <- 50
c(
  param_distributions,
  param_samples,
  fc_means,
  fc_sds,
  smoothed_means,
  smoothed_covs,
  filtered_means,
  filtered_covs
) %<-% fit_vi(
  ts,
  ts_train,
  model,
  n_iterations,
  n_param_samples,
  n_forecast_steps,
  n_forecast_samples
)
```

```{r Everything-Is-Correlated}
#https://www.gwern.net/Everything
#https://lindeloev.github.io/tests-as-linear/
# Load packages for data handling and plotting
library(tidyverse)
library(patchwork)
library(broom)

# Reproducible "random" results
set.seed(40)

# Generate normal data with known parameters
rnorm_fixed = function(N, mu = 0, sd = 1)
  scale(rnorm(N)) * sd + mu

# Plot style.
theme_axis = function(P,
                      jitter = FALSE,
                      xlim = c(-0.5, 2),
                      ylim = c(-0.5, 2),
                      legend.position = NULL) {
  P = P + theme_bw(15) +
    geom_segment(
      x = -1000, xend = 1000,
      y = 0, yend = 0,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    geom_segment(
      x = 0, xend = 0,
      y = -1000, yend = 1000,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    coord_cartesian(xlim = xlim, ylim = ylim) +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      legend.position = legend.position
    )
  
  # Return jittered or non-jittered plot?
  if (jitter) {
    P + geom_jitter(width = 0.1, size = 2)
  }
  else {
    P + geom_point(size = 2)
  }
}
# Wide format (sort of)
#y = rnorm_fixed(50, mu=0.3, sd=2)  # Almost zero mean.
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

```


```{r dimensioN, echo = FALSE}
#http://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/
N=1000
#d<-runif(N)
d<-rep(0,N)
dd<-d
for (n in seq(1:N)) {i=runif(n);d[n]=sum(i);dd[n]=sqrt(sum(i*i))}
hist(d,breaks=N)
plot(d)
plot(dd)
#plot(d,dd)
hist(dd)#,breaks=N)
```

```{r rand-1-10, echo = FALSE}
#https://torvaney.github.io/projects/human-rng
#install.packages("tidyverse")
library(tidyverse)

probabilities <-
  read_csv("https://git.io/fjoZ2") %>%
  count(outcome = round(pick_a_random_number_from_1_10)) %>%
  filter(!is.na(outcome),
         outcome != 0) %>%
  mutate(p = n / sum(n))

probabilities %>%
  ggplot(aes(x = outcome, y = p)) +
  geom_col(aes(fill = as.factor(outcome))) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0, 1, 0.05)) +
  scale_fill_discrete(h = c(120, 360)) +
  theme_minimal(base_family = "Roboto") +
  theme(legend.position = "none",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  labs(title = '"1-10"',
       subtitle = "RNG distribution",
       x = "",
       y = NULL,
       caption = "Source: https://www.reddit.com/r/dataisbeautiful/comments/acow6y/asking_over_8500_students_to_pick_a_random_number/")


variables <-
  crossing(from = probabilities$outcome,
           to   = probabilities$outcome) %>%
  mutate(name = glue::glue("x({from},{to})"),
         ix = row_number())

variables

fill_array <- function(indices,
                       weights,
                       dimensions = c(1, max(variables$ix))) {
  init <- array(0, dim = dimensions)

  if (length(weights) == 1) {
    weights <- rep_len(1, length(indices))
  }

  reduce2(indices, weights, function(a, i, v) {
    a[1, i] <- v
    a
  }, .init = init)
}

constrain_uniform_output <-
  probabilities %>%
  pmap(function(outcome, p, ...) {
    x <-
      variables %>%
      filter(to == outcome) %>%
      left_join(probabilities, by = c("from" = "outcome"))

    fill_array(x$ix, x$p)
  })


one_hot <- partial(fill_array, weights = 1)

constrain_original_conserved <-
  probabilities %>%
  pmap(function(outcome, p, ...) {
    variables %>%
      filter(from == outcome) %>%
      pull(ix) %>%
      one_hot()
  })

maximise_original_distribution_reuse <-
  probabilities %>%
  pmap(function(outcome, p, ...) {
    variables %>%
      filter(from == outcome,
             to == outcome) %>%
      pull(ix) %>%
      one_hot()
  })

objective <- do.call(rbind, maximise_original_distribution_reuse) %>% colSums()


# Make results reproducible...
set.seed(23756434)

#install.packages("lpSolve")
solved <- lpSolve::lp(
  direction    = "max",
  objective.in = objective,
  const.mat    = do.call(rbind, c(constrain_original_conserved, constrain_uniform_output)),
  const.dir    = c(rep_len("==", length(constrain_original_conserved)),
                   rep_len("==", length(constrain_uniform_output))),
  const.rhs    = c(rep_len(1, length(constrain_original_conserved)),
                   rep_len(1 / nrow(probabilities), length(constrain_uniform_output)))
)

balanced_probabilities <-
  variables %>%
  mutate(p = solved$solution) %>%
  left_join(probabilities,
            by = c("from" = "outcome"),
            suffix = c("_redistributed", "_original"))

#install.packages("gganimate")
#install.packages("gifski")gifski_renderer
#install.packages("png")
library(png)
library(gifski)
library(gganimate)

redistribute_anim <-
  bind_rows(balanced_probabilities %>%
            mutate(key   = from,
                   state = "Before"),
            balanced_probabilities %>%
            mutate(key   = to,
                   state = "After")) %>%
  ggplot(aes(x = key, y = p_redistributed * p_original)) +
  geom_col(aes(fill = as.factor(from)),
           position = position_stack()) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0, 1, 0.05)) +
  scale_fill_discrete(h = c(120, 360)) +
  theme_minimal(base_family = "Roboto") +
  theme(legend.position = "none",
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  labs(title = 'Balancing RNG distribution"',
       subtitle = "{closest_state}",
       x = "",
       y = NULL) +
  transition_states(
    state,
    transition_length = 4,
    state_length = 3
  ) +
  ease_aes('cubic-in-out')

animate(
  redistribute_anim,
  start_pause = 8,
  end_pause = 8
)

balanced_probabilities %>%
  ggplot(aes(x = from, y = to)) +
  geom_tile(aes(alpha = p_redistributed, fill = as.factor(from))) +
  geom_text(aes(label = ifelse(p_redistributed == 0, "", scales::percent(p_redistributed, 2)))) +
  scale_alpha_continuous(limits = c(0, 1), range = c(0, 1)) +
  scale_fill_discrete(h = c(120, 360)) +
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(breaks = 1:10) +
  theme_minimal(base_family = "Roboto") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_line(linetype = "dotted"),
        legend.position = "none") +
  labs(title = "Probability mass redistribution",
       x = "Original number",
       y = "Redistributed number")

probabilities %>%
  transmute(number = outcome,
            probability = scales::percent(p))


```

```{r compMS, echo = FALSE}
install.packages('rlang')
library(devtools)
install_github('coltocaro/compMS')
setRepositories(graphics = getOption("menu.graphics"),ind = NULL, addURLs = character())
library(compMS)
```


```{r tfp, echo = FALSE}
#https://blogs.rstudio.com/tensorflow/posts/2019-06-25-dynamic_linear_models_tfprobability/
#install.packages("devtools")
#devtools::install_github("rstudio/tfprobability")
library(tensorflow)
#tensorflow::install_tensorflow(version = "1.14")
library(tfprobability)
install.packages("tidyverse")
library(tidyverse)
library(zeallot)
df <- read_table("~/Documents/capm.txt",col_types = list(X1 = col_date(format = "%Y.%m"))) %>%  rename(month = X1)
df %>% glimpse()
df %>% gather(key = "symbol", value = "return", -month) %>%
  ggplot(aes(x = month, y = return, color = symbol)) +
  geom_line() +
  facet_grid(rows = vars(symbol), scales = "free")
ibm <- df$IBM - df$RKFREE
# market excess returns
x <- df$MARKET - df$RKFREE

fit <- lm(ibm ~ x)
summary(fit)
ts <- ibm %>% matrix()
# forecast 12 months
n_forecast_steps <- 12
ts_train <- ts[1:(length(ts) - n_forecast_steps), 1, drop = FALSE]

# make sure we work with float32 here
ts_train <- tf$cast(ts_train, tf$float32)
ts <- tf$cast(ts, tf$float32)
linreg <- ts %>%
  sts_dynamic_linear_regression(
    design_matrix = cbind(rep(1, length(x)), x) %>% tf$cast(tf$float32)
  )

optimizer <- tf$compat$v1$train$AdamOptimizer(0.1)
# only train on the training set!    
loss_and_dists <- ts_train %>% sts_build_factored_variational_loss(model = model)
variational_loss <- loss_and_dists[[1]]

train_op <- optimizer$minimize(variational_loss)

```

```{r shiny, echo = FALSE}
#https://rmarkdown.rstudio.com/authoring_shiny.html
#install.packages('shiny')
library(shiny)
dN_shiny <- dataNorm
#ctrl-shift-K
#rmarkdown::run
#shiny::renderUI
```

```{r plotSVG}
library(ggplot2)
fileP<-"L:/promec/Animesh/api5000/"
fileS<-list.files(path=fileP, pattern="*.txt", full.names=TRUE, recursive=FALSE)
for(i in fileS){
  print(i)
  data <- read.table(i,sep="\t", header=F)
  print(summary(data))
  plot(data,type = "l")  #print(ggplot(data)+aes(data$V1,data$V2))
  ggsave(file=paste0(i,".svg"), plot=plot(data,type = "l"), width=6, height=6)
}
```

```{r ANOVA,echo=F}
pathD<-"F:/promec/USERS/MarianneNymark/181009/PDv2p3/181009_newprep_Charlotte_Alb3b-14_II"
inpF<-file.path(pathD,"181009_newprep_Charlotte_Alb3b-14_II-(1)_Proteins.txt")
data<-read.table(inpF,header=T,sep="\t",row.names = 3)
#summary(data)

inpL<-"F:/promec/USERS/MarianneNymark/181009/PDv2p3/181009_newprep_Charlotte_Alb3b-14_II/Groups.txt"
label<-read.table(inpL,header=T,sep="\t")
#colnames(label)
#summary(label)

y<-log2(as.matrix(data[32:46]))
#summary(y)
#hist(y)
row.names(y)<-row.names.data.frame(data)
y[is.na(y)]<-0
colnames(y)=sub("Abundances.Normalized.F","",colnames(y))
colnames(y)=sub(".Sample","",colnames(y))
#summary(y)

replicate<-as.factor(label$Replicate)
class<-as.factor(label$Group)

dataNorm<-y
set.seed(1)
dataNorm[dataNorm==0]<-rnorm(1,mean=mean(y),sd=sd(y))
#summary(dataNorm)

#TukeyHSD(aov(dataNorm["B7G7S4",]~class),"class", ordered = TRUE)[["class"]][10:12]

#chkANOVA<-apply(dataNorm,1,function(x){TukeyHSD(aov(x~class),"class")})
#chkANOVAnames<-t(sapply(row.names(dataNorm),function(x){chkANOVA[[x]]$`class`[10:12]}))
#chkANOVAnames<-apply(chkANOVAnames,2,function(x){p.adjust(x,"BH")})
#colnames(chkANOVAnames)<-c("Alb3b-16-Alb3b-14","Alb3b-14-WT", "Alb3b-16-WT")
#Uniprot<-sapply(strsplit(row.names(chkANOVAnames),";"), `[`, 1)
#write.csv(cbind(chkANOVAnames,Uniprot),file.path(pathD,"chkANOVAnames.csv"))
```



```{r slider, echo=FALSE}

inputPanel(
  sliderInput("bins", "#bins:", min = 1, max = 100, value = 30),
  textInput("dens", "Density:", "auto",value = 0.6), 
  selectInput("expression", label = "Sample:",choices = colnames(dN_shiny), selected = colnames(dN_shiny)[1]))
renderText({paste("Inputs:", input$bins,input$dens,input$expression)})
```

```{r plot, echo = FALSE}
renderPlot({
  hist(dN_shiny[,as.numeric(which(colnames(dN_shiny) == input$expression))],breaks = as.numeric(input$bins),probability = TRUE,col="orange")
  lines(density(dN_shiny[,as.numeric(which(colnames(dN_shiny) == input$expression))], adjust = as.numeric(input$dens)),col="blue")
  })
  
```



```{r SiB-Workshop - Day 4 – deep-learning - test - diabetes-data}
fileP<-"/home/animeshs/Downloads/"
fileN<-"diabetes.csv"
yy<-read.csv(paste0(fileP, fileN),header=T)
summary(yy[,9])
#diab[,9]=as.numeric(diab[,9]-1)
plot(yy,col=yy$Outcome,main="diabetes")

yyt=as.matrix(yy)
#yyt[,9]=as.factor(yyt[,9])
library(ggfortify)
log.yyt=log(yyt[,1:8]+1)
yyt.pca=prcomp(log.yyt,center=TRUE,scale.=TRUE) 
autoplot(yyt.pca,data=yyt,colour='Outcome',main="dataset")

dimnames(yyt)=NULL
summary(yyt)


library(keras)
#install_keras()
use_session_with_seed(2)
ind=sample(2,nrow(yyt),replace=TRUE,prob=c(0.75,0.25))

yyt.training=yyt[ind==1,1:8]
yyt.test=yyt[ind==2,1:8]

yyt.trainingtarget=yyt[ind==1,9]
yyt.testtarget=yyt[ind==2,9]

yyt.trainLabels=to_categorical(yyt.trainingtarget)
yyt.testLabels=to_categorical(yyt.testtarget)
```

```{r SiB-Workshop - Day 4 – deep-learning - test - diabetes-data - run}
model=keras_model_sequential()
model %>%
    layer_dense(input_shape=c(8),units=8,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>%
#    layer_dense(input_shape=c(6),units=6) %>%
#    layer_dense(input_shape=c(4),units=4) %>%
    layer_dense(units=2,activation='softmax',kernel_initializer="glorot_normal",use_bias=TRUE)
summary(model)

model %>% compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
history=model %>% fit(yyt.training,yyt.trainLabels,epochs=100,batch_size=nrow(yyt.training),validation_split=0.2)
#history=model %>% fit(yyt.training,yyt.trainLabels,epochs=100,batch_size=5,validation_split=0.2)

plot(history)
predicted.classes=model %>% predict_classes(yyt.test)
table(yyt.testtarget,predicted.classes)
```

```{r read - lym-data}
fileP<-"/home/animeshs/Downloads/"
fileN<-"Sel66.txt"
lym<-read.table(paste0(fileP, fileN),row.names = 1, header=T)
fileC<-"Code.txt"
code<-read.table(paste0(fileP, fileC),row.names = 1, header=T)
summary(lym)
library(quantable)
y<-lym
y=robustscale(y)
y$data[is.na(y$data)]<-0
names(y$data)=sub("X","",names(y$data))
colnames(y$data)
yy<-rbind(y$data,code$Code)
yyt<-t(yy)
colnames(yyt) <- gsub(";", "_", colnames(yyt))
colnames(yyt) <- gsub("-", "__", colnames(yyt))
summary(yyt)
#yyt[,67]=as.numeric(yyt[,67]-1)
colnames(yyt)[67] <- "class"
summary(yyt)
plot(yyt[,67])
```

```{r SiB-Workshop - Day 4 – tree - lym}
library("rpart")
library("rpart.plot")
library("randomForest")
set.seed(100)
yyt=as.data.frame(yyt)
colnames(yyt)[67] <- "class"
yyt[,67]=as.factor(yyt[,67])
ind=sample(2,nrow(yyt),replace=TRUE,prob=c(0.80,0.20))
yyt.training=yyt[ind==1,]
yyt.test=yyt[ind==2,]

tree=rpart(data=yyt.training,class~.,method="class",control=rpart.control(minsplit=10,minbucket=5),parms=list(split="information"))
rpart.plot(tree,main="Classification tree for the yyt data (using 80% of data as training set)",extra=101)

#library(ggplot2)
#qplot(PTBP2,GALNT2,data=yyt,colour=class,size=I(3))
```


```{r SiB-Workshop - Day 4 – tree - lym - pred}
predictions=predict(tree,newdata=yyt.training,type="class")
actuals=yyt.training$class
table(actuals,predictions)

predictions=predict(tree,newdata=yyt.test,type="class")
actuals=yyt.test$class
confusion.matrix=table(actuals,predictions)
print(confusion.matrix)

accuracy=sum(diag(confusion.matrix))/sum(confusion.matrix)
print(accuracy)

tree=rpart(data=yyt,class~.,method="class",control=rpart.control(minsplit=1,minbucket=1,cp=0.000001),parms=list(split="information"))
rpart.plot(tree,main="Biggest Tree",extra=101)

printcp(tree)

plotcp(tree)

ptree=prune(tree,cp=2.0e-02)
rpart.plot(ptree,main="Pruned Tree",extra=101)
#qplot(ALDH1B1,PTBP2,data=yyt,colour=class,size=I(3))
```

```{r SiB-Workshop - Day 4 – randomtree - lym - pred}
library(randomForest)
set.seed(100)
#yyt=as.data.frame(yyt)
random_forest=randomForest(data=yyt.training,class~.,impurity='gini',ntree=200,replace=TRUE)
print(random_forest)

plot(random_forest)
legend("top",cex=0.8,legend=colnames(random_forest$err.rate),lty=c(1,2,3),col=c(1,2,3),horiz=T)

random_forest=randomForest(data=yyt.training,class~.,impurity='gini',ntree=25,replace=TRUE)

predictions=predict(random_forest,newdata=yyt.training,type="class")
actuals=yyt.training$class
table(actuals,predictions)

predictions=predict(random_forest,newdata=yyt.test,type="class")
actuals=yyt.test$class
confusion.matrix=table(actuals,predictions)
print(confusion.matrix)

accuracy=sum(diag(confusion.matrix))/sum(confusion.matrix)
print(accuracy)

sort(importance(random_forest))
varImpPlot(random_forest)
#qplot(RCN1,GALNT2,data=yyt,colour=class,size=I(3))
```

```{r SiB-Workshop - Day 4 – tree}
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("randomForest")
library("rpart")
library("rpart.plot")
library("randomForest")
data(iris)
set.seed(2)
ind=sample(2,nrow(iris),replace=TRUE,prob=c(0.80,0.20))
iris.training=iris[ind==1,]
iris.test=iris[ind==2,]
tree=rpart(data=iris.training,Species~Sepal.Width+Sepal.Length+Petal.Length+Petal.Width,method="class",control=rpart.control(minsplit=10,minbucket=5),parms=list(split="information"))
rpart.plot(tree,main="Classification tree for the iris data (using 80% of data as training set)",extra=101)

library(ggplot2)
qplot(Petal.Length,Petal.Width,data=iris,colour=Species,size=I(3))
predictions=predict(tree,newdata=iris.training,type="class")
actuals=iris.training$Species
table(actuals,predictions)

predictions=predict(tree,newdata=iris.test,type="class")
actuals=iris.test$Species
confusion.matrix=table(actuals,predictions)
print(confusion.matrix)

accuracy=sum(diag(confusion.matrix))/sum(confusion.matrix)
print(accuracy)

tree=rpart(data=iris,Species~Sepal.Width+Sepal.Length+Petal.Length+Petal.Width,method="class",control=rpart.control(minsplit=1,minbucket=1,cp=0.000001),parms=list(split="information"))
rpart.plot(tree,main="Biggest Tree",extra=101)

printcp(tree)

plotcp(tree)

ptree=prune(tree,cp=2.0e-02)
rpart.plot(ptree,main="Pruned Tree",extra=101)

library(randomForest)

random_forest=randomForest(data=iris.training,Species~Sepal.Width+Sepal.Length+Petal.Length+Petal.Width,impurity='gini',ntree=200,replace=TRUE)
print(random_forest)

plot(random_forest)
legend("top",cex=0.8,legend=colnames(random_forest$err.rate),lty=c(1,2,3),col=c(1,2,3),horiz=T)

random_forest=randomForest(data=iris.training,Species~Sepal.Width+Sepal.Length+Petal.Length+Petal.Width,impurity='gini',ntree=25,replace=TRUE)

predictions=predict(random_forest,newdata=iris.training,type="class")
actuals=iris.training$Species
table(actuals,predictions)

predictions=predict(random_forest,newdata=iris.test,type="class")
actuals=iris.test$Species
confusion.matrix=table(actuals,predictions)
print(confusion.matrix)

accuracy=sum(diag(confusion.matrix))/sum(confusion.matrix)
print(accuracy)

importance(random_forest)
varImpPlot(random_forest)

```


```{r SiB-Workshop - Day 3 – deep-learning - lym}
fileP<-"/home/animeshs/Downloads/"
fileN<-"Sel66.txt"
lym<-read.table(paste0(fileP, fileN),row.names = 1, header=T)
fileC<-"Code.txt"
code<-read.table(paste0(fileP, fileC),row.names = 1, header=T)
summary(lym)
library(quantable)
y<-lym
y=robustscale(y)
y$data[is.na(y$data)]<-0
names(y$data)=sub("X","",names(y$data))
colnames(y$data)
yy<-rbind(y$data,code$Code)

yyt<-t(yy)
summary(yyt)
yyt[,67]=as.numeric(yyt[,67]-1)
summary(yyt)
plot(yyt[,67])
yyt=as.matrix(yyt)
dimnames(yyt)=NULL

library(keras)
#install_keras()
use_session_with_seed(3)
set.seed(2)
ind=sample(2,nrow(yyt),replace=TRUE,prob=c(0.80,0.20))

yyt.training=yyt[ind==1,1:66]
yyt.test=yyt[ind==2,1:66]

yyt.trainingtarget=yyt[ind==1,67]
yyt.testtarget=yyt[ind==2,67]

yyt.trainLabels=to_categorical(yyt.trainingtarget)
yyt.testLabels=to_categorical(yyt.testtarget)
model=keras_model_sequential()
model %>%
    layer_dense(input_shape=c(66),units=33,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>%
    layer_dense(units=3,activation='softmax',kernel_initializer="glorot_normal",use_bias=TRUE)
summary(model)

model %>% compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
history=model %>% fit(yyt.training,yyt.trainLabels,epochs=100,batch_size=5,validation_split=0.2)

plot(history)
predicted.classes=model %>% predict_classes(yyt.test)
table(yyt.testtarget,predicted.classes)
```

```{r SiB-Workshop - Day 3 – deep-learning}
#install.packages("devtools")
#devtools::install_github("rstudio/keras")
library(keras)
#install_keras()
use_session_with_seed(3)
data(iris)
head(iris, 3)
summary(iris)
plot(iris)
#install.packages("ggplot2")
#install.packages("ggfortify")
library("ggfortify")
log.iris=log(iris[,1:4])
iris.pca=prcomp(log.iris,center=TRUE,scale.=TRUE) 
library("ggplot2")
autoplot(iris.pca,data=iris,colour='Species',main="PCA of the iris dataset")

plot(iris$Sepal.Length,iris$Sepal.Width,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],xlab="Sepal Length",ylab="Sepal Width")

plot(iris$Petal.Length,iris$Petal.Width,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],xlab="Petal Length",ylab="Petal Width")

data(iris)
iris[,5]=as.numeric(iris[,5])-1
iris=as.matrix(iris)
dimnames(iris)=NULL

set.seed(2)
ind=sample(2,nrow(iris),replace=TRUE,prob=c(0.80,0.20))

iris.training=iris[ind==1,1:4]
iris.test=iris[ind==2,1:4]

iris.trainingtarget=iris[ind==1,5]
iris.testtarget=iris[ind==2,5]

iris.trainLabels=to_categorical(iris.trainingtarget)
iris.testLabels=to_categorical(iris.testtarget)
model=keras_model_sequential()
model %>%
    layer_dense(input_shape=c(4),units=8,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>%
    layer_dense(units=3,activation='softmax',kernel_initializer="glorot_normal",use_bias=TRUE)
summary(model)

model %>% compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
history=model %>% fit(iris.training,iris.trainLabels,epochs=100,batch_size=5,validation_split=0.2)

plot(history)

predicted.classes=model %>% predict_classes(iris.test)
table(iris.testtarget,predicted.classes)

score=model %>% evaluate(iris.test,iris.testLabels)
print(score)
```

```{r SiB-Workshop - Day 3 – deep-learning - test}


model=keras_model_sequential() 

model %>% 
    layer_dense(input_shape=c(4),units=28,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>% 
    layer_dense(units=3,activation='softmax',kernel_initializer="glorot_normal",use_bias=TRUE)

model %>% compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')

model %>% fit(iris.training,iris.trainLabels,epochs=200,batch_size=5,validation_split=0.2)

score=model %>% evaluate(iris.test,iris.testLabels)

print(score)

model=keras_model_sequential() 
model %>% 
    layer_dense(input_shape=c(4),units=28,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>% 
    layer_dense(units=5,activation='relu',kernel_initializer="glorot_normal",use_bias=TRUE) %>% 
    layer_dense(units=3,activation='softmax',kernel_initializer="glorot_normal",use_bias=TRUE)
model %>% compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')
model %>% fit(iris.training,iris.trainLabels,epochs=200,batch_size=5,validation_split=0.2)
score=model %>% evaluate(iris.test,iris.testLabels)
print(score)
```



```{r SiB-Workshop - Day 3 – clust}
?hclust
?kmeans
mat <- matrix(data = rnorm(30, mean= 100,sd=10), nrow = 15, ncol = 2)
mat.dist<-as.matrix(dist(mat))
heatmap(mat.dist)
heatmap(mat.dist,Colv=NA, Rowv=NA, scale="none")
colorScale <- colorRampPalette(c("blue","green","yellow","red","darkred"))(1000)

heatmap(mat.dist,Colv=NA, Rowv=NA,scale="none",col=colorScale)

fileP<-"/home/animeshs/Downloads/"
fileN<-"Sel66.txt"
inputFile <- paste0(fileP, fileN)
lym<-read.table(inputFile,row.names = 1, header=T)
mat.dist<-as.matrix(dist(t(lym)))
heatmap(mat.dist)
cl<- kmeans(lym, 10, iter.max=20)
plot(cl$centers)
cah <- hclust(dist(cl$centers))#,"complete")#, graph=FALSE, nb.clust=-1)
plot(cah)


distE<- dist(mat)
distC<- dist(mat,method="manhattan")
mat.distE<-as.matrix(dist(mat))
mat.distC<-as.matrix(dist(mat,method="manhattan"))
heatmap(mat.distE,Colv=NA, Rowv=NA,scale="none",col=colorScale)
heatmap(mat.distC,Colv=NA, Rowv=NA,scale="none",col=colorScale)
hE<-hclust(distE,"complete")
hC<-hclust(distC,"complete")
plot(hE)
plot(hC)

df<-data.frame(mat)
kmeans(df,3)
cl.1 <- kmeans(df, 3, iter.max = 1)
plot(df, col = cl.1$cluster)
points(cl.1$centers, col = 1:5, pch = 8)

cl.1000 <- kmeans(df, 3, iter.max = 1000)
plot(df, col = cl.1000$cluster)
points(cl.1000$centers, col = 1:5, pch = 8)

x<- rbind(matrix(rnorm(70000, sd = 0.3), ncol = 2),matrix(rnorm(70000, mean = 1, sd = 0.3),ncol = 2))
colnames(x) <- c("x", "y")
cl<- kmeans(x, 1000, iter.max=20)
plot(cl$centers)
cah <- hclust(dist(cl$centers))#,"complete")#, graph=FALSE, nb.clust=-1)
plot(cah)

fileP<-"/home/animeshs/Downloads/"
fileN<-"Sel66.txt"
inputFile <- paste0(fileP, fileN)
lym<-read.table(inputFile,row.names = 1)
cl<- kmeans(lym, 10, iter.max=20)
plot(cl$centers)
cah <- hclust(dist(cl$centers))#,"complete")#, graph=FALSE, nb.clust=-1)
plot(cah)

clustering_data<- rxImport(inData = inputFile)
rxGetVarInfo(clustering_data)

z<-rxKmeans(~1_19912 + ~9_1702, data =clustering_data, numClusters = 3, maxIterations=100)

lym<-read.table(inputFile,row.names = 1,header = T)
summary(lym)
clustering_data<-data.frame(lym)
#clustering_data<- rxImport(inData = "/home/animeshs/Downloads/dataClustering.csv")
rxGetVarInfo(clustering_data)
z<-rxKmeans(~X9_1702 + X8_16846, data =clustering_data, numClusters = 3, maxIterations=100)
z<-rxKmeans(as.formula(paste("~",paste(names(clustering_data),collapse="+"))),data = clustering_data, numClusters = 3, maxIterations=100)
#plot(clustering_data, col = z$cluster)
points(z$centers, col = 1:5, pch = 8)

```

```{r SiB-Workshop - Day 2 – mclus}
#install.packages("mclust")
library("mclust")
?mclustBIC
?Mclust
fileP<-"/home/animeshs/Downloads/"
fileN<-"Sel66.txt"
inputFile <- paste0(fileP, fileN)
lym<-read.table(inputFile,row.names = 1,header=T)
summary(lym)
lym[is.na(lym)]=0
BIC <- mclustBIC(lym)
plot(BIC)
summary(BIC)
mod1 <- Mclust(lym, x = BIC)
summary(mod1, parameters = TRUE)
plot(mod1, what = "classification")
```

```{r SiB-Workshop - Day 2 – pca}
pcaD<-prcomp(lym, center = TRUE, scale. = FALSE)
summary(pcaD)
plot(pcaD$x)
```

```{r SiB-Workshop - Day 2 – lda}
library(MASS)
data(iris)
head(iris, 3)
train <- sample(1:150, 75)
r <- lda(formula = Species ~ .,data = iris,prior = c(1,1,1)/3,subset = train)
r$prior
r$counts
#means for each covariate
r$means
#with 3 classes we have at most two linear discriminants
r$scaling
#the singular values (svd) that gives the ratio of the between- and within-group standard deviations on the linear discriminan variables.
r$svd
# amount of the between-group variance that is explained by eaclinear discriminant
prop = r$svd^2/sum(r$svd^2)
head(prop$class)
head(r2$posterior, 3)
plda = predict(object = r, newdata = iris[-train, ])
```

```{r SiB-Workshop - Day 2 – RevoScaleR}
#<pre>export RSTUDIO_WHICH_R=/usr/bin/Revo64</pre>
#export R_LIBS_SITE=/opt/microsoft/rclient/3.4.3/libraries/RServer
#sudo ln -s /usr/lib/x86_64-linux-gnu/libpng16.so.16.36.0 /usr/lib/x86_64-linux-gnu/libpng12.so.0

   inputFileFlight <- paste0("/home/animeshs/Downloads/", "Flight_Delays_Sample.csv")
   inputFileWeather <- paste0("/home/animeshs/Downloads/", "Weather_Sample.csv")

#Create a temporary directory to store the intermediate XDF files. 

   outFileFlight  <- "/home/animeshs/Downloads/flight.xdf"
   outFileWeather <- "/home/animeshs/Downloads/weather.xdf"
   outFileOrigin <-  "/home/animeshs/Downloads/originData.xdf"
   outFileDest   <-  "/home/animeshs/Downloads/destData.xdf"
   outFileFinal  <-  "/home/animeshs/Downloads/finalData.xdf"

#Import the flight data.
    flight_mrs <- rxImport(
      inData = inputFileFlight, outFile = outFileFlight,
      missingValueString = "M", stringsAsFactors = FALSE,
      # Remove columns that are possible target leakers from the flight data.
      varsToDrop = c("DepDelay", "DepDel15", "ArrDelay", "Cancelled", "Year"),
      # Define "Carrier" as categorical.
      colInfo = list(Carrier = list(type = "factor")),
      # Round down scheduled departure time to full hour.
      transforms = list(CRSDepTime = floor(CRSDepTime/100)),  
      overwrite = TRUE
    )

#Review the first six rows of flight data.
    head(flight_mrs)

#Summarize the flight data.
    rxSummary(~., data = flight_mrs, blocksPerRead = 2)

#Import the weather data.
    xform <- function(dataList) {
      # Create a function to normalize some numerical features.
      featureNames <- c(
        "Visibility", 
        "DryBulbCelsius", 
        "DewPointCelsius", 
        "RelativeHumidity", 
        "WindSpeed", 
        "Altimeter"
      )
      dataList[featureNames] <- lapply(dataList[featureNames], scale)
      return(dataList)
    }

    weather_mrs <- rxImport(
      inData = inputFileWeather, outFile = outFileWeather,
      missingValueString = "M", stringsAsFactors = FALSE,
      # Eliminate some features due to redundance.
      varsToDrop = c("Year", "Timezone", 
                     "DryBulbFarenheit", "DewPointFarenheit"),
      # Create a new column "DestAirportID" in weather data.
      transforms = list(DestAirportID = AirportID),
      # Apply the normalization function.
      transformFunc = xform,  
      transformVars = c(
        "Visibility", 
        "DryBulbCelsius", 
        "DewPointCelsius", 
        "RelativeHumidity", 
        "WindSpeed", 
        "Altimeter"
      ),
      overwrite = TRUE
    )

#Review the variable information for the weather data.
    rxGetVarInfo(weather_mrs)


#Step 2: Pre-process Data
#Prepare for a merge by renaming some columns in the weather data.
    newVarInfo <- list(
      AdjustedMonth = list(newName = "Month"),
      AdjustedDay = list(newName = "DayofMonth"),
      AirportID = list(newName = "OriginAirportID"),
      AdjustedHour = list(newName = "CRSDepTime")
    )
    rxSetVarInfo(varInfo = newVarInfo, data = weather_mrs)

#Concatenate/Merge flight records and weather data.
##Join flight records and weather data at origin of the flight `OriginAirportID`.
      originData_mrs <- rxMerge(
        inData1 = flight_mrs, inData2 = weather_mrs, outFile = outFileOrigin,
        type = "inner", autoSort = TRUE, 
        matchVars = c("Month", "DayofMonth", "OriginAirportID", "CRSDepTime"),
        varsToDrop2 = "DestAirportID",
        overwrite = TRUE
      )

##Join flight records and weather data using the destination of the flight `DestAirportID`.
      destData_mrs <- rxMerge(
        inData1 = originData_mrs, inData2 = weather_mrs, outFile = outFileDest,
        type = "inner", autoSort = TRUE, 
        matchVars = c("Month", "DayofMonth", "DestAirportID", "CRSDepTime"),
        varsToDrop2 = c("OriginAirportID"),
        duplicateVarExt = c("Origin", "Destination"),
        overwrite = TRUE
      )

##Call the rxFactors() function to convert `OriginAirportID` and `DestAirportID` as categorical.
      rxFactors(inData = destData_mrs, outFile = outFileFinal, sortLevels = TRUE,
                factorInfo = c("OriginAirportID", "DestAirportID"),
                overwrite = TRUE)
rxGetInfo(outFileFinal)
rxGetInfo(outFileFinal,getVarInfo=TRUE)
```

```{r SiB-Workshop - Day 2 - Example 2 – Basic Regression}
data <- read.table("/home/animeshs/Downloads/class.txt")
summary(data)
pairs(data)
model <- lm( Height ~ Age , data = data)
summary(model)
lm_class_basicR<-lm(formula= Height~Weight, data = data[,-1])
summary(lm_class_basicR)

class_data<-rxImport(inData = "/home/animeshs/Downloads/class.txt")
lm_class<-rxLinMod(formula= Height~Weight, data =class_data[,-1])
summary(lm_class)
summary(lm_class_basicR)


big_data<-rxImport(inData = outFileFinal)
lm_class_big<-rxLinMod(formula= WindSpeed.Destination~Altimeter.Destination, data =big_data)
summary(lm_class_big)
#scatter(~WindSpeed.Destination,~Altimeter.Destination,data=outFileFinal)
rxHistogram(~WindSpeed.Destination,data=outFileFinal)

lm_class_big_basicR<-lm(WindSpeed.Destination~Altimeter.Destination, data =big_data)
summary(lm_class_big_basicR)
lm_class_gender<-rxLinMod(formula= Height~Age+Gender, data =recodedDF2[,-1])
summary(lm_class_gender)
#scatter(~WindSpeed.Destination,~Altimeter.Destination,data=outFileFinal)
rxHistogram(~WindSpeed.Destination,data=outFileFinal)

recodedDF2 <- rxFactors(inData = class_data, sortLevels = TRUE,factorInfo = c("Gender"))   
rxGetVarInfo(recodedDF2)
lm_class_gender<-rxLinMod(formula= Height~Age+Gender,data =recodedDF2[,-1])
summary(lm_class_gender)
```

```{r cheeseTest}
cheese_data<-rxImport(inData = "/home/animeshs/Downloads/cheese.txt")
rxGetVarInfo(cheese_data)
lm_taste<-rxLinMod(formula= taste~Acetic, data =cheese_data[,-1])
summary(lm_taste)
lm_taste<-rxLinMod(formula= taste~Lactic, data =cheese_data[,-1])
summary(lm_taste)
lm_taste<-rxLinMod(formula= taste~H2S, data =cheese_data[,-1])
summary(lm_taste)
lm_taste<-rxLinMod(formula= taste~Case, data =cheese_data[,-1])
summary(lm_taste)
lm_taste<-rxLinMod(formula= taste~Acetic*Lactic, data =cheese_data[,-1])
summary(lm_taste)
lm_taste<-rxLinMod(formula= taste~Acetic+Lactic, data =cheese_data[,-1])
summary(lm_taste)
head(cheese_data)
rxHistogram(~Case,data=cheese_data)
recodedDF2 <- rxFactors(inData = cheese_data, sortLevels = TRUE,factorInfo = c("Case"))   
rxGetVarInfo(recodedDF2)
lm_taste_case<-rxLinMod(formula= taste~Acetic+Lactic+Case, data =recodedDF2[,-1])
summary(lm_taste_case)
rxHistogram(~taste,data=cheese_data)
rxHistogram(~H2S,data=cheese_data)
cheese_data_logH2S<-rxDataStep(inData=cheese_data,transforms = list(logH2S = log(cheese_data[,5])),overwrite = TRUE)
hist(log(cheese_data[,5]))
lm_taste_case<-rxLinMod(formula= taste~H2S, data =recodedDF2[,-1])
summary(lm_taste_case)
lm_taste_case<-lm(formula= taste~log(H2S), data =recodedDF2[,-1])
summary(lm_taste_case)
```

```{r logit}
class_data_basic<-read.table("/home/animeshs/Downloads/class.txt")
logitmodel_basic<-glm(Gender~Height,family=binomial,data=class_data_basic)
summary(logitmodel_basic)
class_data<-rxImport(inData = "/home/animeshs/Downloads/class.txt")
recodedDF <- rxFactors(inData = class_data, sortLevels = TRUE,factorInfo = c("Gender"))   
rxGetVarInfo(recodedDF)
logitmodel<-rxLogit(Gender~Height,data=recodedDF[,-1])
summary(logitmodel)
rxHistogram(~Height,data=recodedDF[,-1])
```

```{r logit-brfss}
#brfss_data<-rxImport(inData = "/home/animeshs/LLCP2013.XPT")
brfss<-read.csv("/home/animeshs/Downloads/LLCP2013_sampled.csv")
brfss$has_plan <- brfss$HLTHPLN1 == 1
brfss$X.RACE
summary(glm(has_plan ~ as.factor(brfss$X.RACE), data=brfss,family=binomial))
recodedDF2 <-brfss
recodedDF2 <- rxFactors(inData = brfss, sortLevels = TRUE,factorInfo = c("X.RACE"))   
str(recodedDF2$X.RACE)
recodedDF2$HLTHPLN1_has<- recodedDF2$HLTHPLN1 == 1
logitmodel_brfss<-rxLogit(has_plan~X.RACE,data=recodedDF2[,-1])
logitmodel_brfss<-rxLogit(HLTHPLN1_has~X.RACE,data=recodedDF2[,-1])
summary(logitmodel_brfss)
levels(as.factor(brfss$X.RACE))
levels(recodedDF2$X.RACE)
       
brfss_data<-rxImport(inData = "/home/animeshs/Downloads/LLCP2013_sampled.csv")
rxGetVarInfo(brfss_data)
recodedDF2 <- rxFactors(inData = brfss_data, sortLevels = TRUE,factorInfo = c("X.RACE"))   
recodedDF2$HLTHPLN1_has<- recodedDF2$HLTHPLN1 == 1
recodedDF2$has_plan<- recodedDF2$HLTHPLN1 == 1
str(recodedDF2$X.RACE)
str(recodedDF2$has_plan)
rxGetVarInfo(recodedDF2)
rxHistogram(~X.RACE,data=recodedDF2[,-1])
rxHistogram(~HLTHPLN1,data=recodedDF2[,-1])
rxHistogram(~HLTHPLN1_has,data=recodedDF2[,-1])
#complete.cases(recodedDF2)
logitmodel_brfss<-rxLogit(HLTHPLN1_has~X.RACE,data=recodedDF2[,-1])
#logitmodel_brfss<-rxLogit(HHLTHPLN1_has~X.RACE,data=recodedDF2[,-1],family=binomial())
summary(logitmodel_brfss)
logitmodel_brfss<-rxLogit(has_plan~X.RACE,data=recodedDF2[,-1])
#logitmodel_brfss<-rxLogit(HHLTHPLN1_has~X.RACE,data=recodedDF2[,-1],family=binomial())
summary(logitmodel_brfss)
valP<-(glm(has_plan ~ as.factor(brfss$X.RACE), data=brfss,family=binomial))
valP[[7]]$tol
pv<-c(0.00346, 0.94744, 0.97384, 0.98888, 0.18799, 0.05449, 0.27669)
p.adjust(pv, method = "hommel", n = length(pv))
install.packages("caret")
library("caret")

```

```{r Revox64}
#library("RevoScaleR")
data<-rxImport(inData='/home/animeshs/Downloads/mortality.csv')
rxGetInfo(data,getVarInfo=TRUE,numRows=3)
dataStep<-rxImport(inData='/home/animeshs/Downloads/mortality.csv',outFile='/home/animeshs/Downloads/mortalityDS.txt',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850)
rxGetInfo(dataStep,getVarInfo=TRUE)
dataStep<-rxImport(inData='/home/animeshs/Downloads/mortality.csv',outFile='/home/animeshs/Downloads/mortalityDS.gz',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850,transforms = list(expression = cut(ccExp, breaks = c(0, 6500, 13000),labels = c("Low exp", "High exp"))))
dataStep<-rxImport(inData='/home/animeshs/Downloads/mortality.csv',outFile='/home/animeshs/Downloads/mortalityDS.gz',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850,transforms = list(expression = floor(ccExp/100)))
rxHistogram(~score,data=dataStep)
rxHistogram(~expression,data=dataStep)
dataStep<-rxImport(inData='/home/animeshs/Downloads/mortality.csv',outFile='/home/animeshs/Downloads/mortalityDS.gz',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850,transforms = list(expression = floor(ccExp/100)))
dataStep<-rxDataStep(inData='/home/animeshs/Downloads/mortality.csv',outFile='/home/animeshs/Downloads/mortalityDS.gz',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850,transforms = list(expression = floor(ccExp/100)))

datacsv <- rxImport("/home/animeshs/Downloads/mortality.csv")
dataExampleNew1<-rxDataStep(inData=dataStep,outFile='/home/animeshs/Downloads/mortalityDS.gz',varsToDrop=c("year"),overwrite=TRUE, rowSelection=score<850,transforms = list(expression = floor(ccExp/100)))
dataExampleNew1<-rxDataStep(inData=dataStep,transforms = list(expression = floor(ccExp/100)))
dataExampleNew2<-rxDataStep(inData=dataStep,transforms = list(expression = floor(ccExp/100)))

dataFD <- rxImport("/home/animeshs/Downloads/Flight_Delays_Sample.csv")
dataWS <- rxImport("/home/animeshs/Downloads/Weather_Sample.csv")
rxGetInfo(dataFD,getVarInfo=TRUE)
rxGetInfo(dataWS,getVarInfo=TRUE)
dataWSdesport<-rxDataStep(inData=dataWS,transforms = list(OriginAirportID = AirportID))
rxGetInfo(dataWSdesport,getVarInfo=TRUE)
dataWSdesport<-rxDataStep(inData=dataWS,transforms = list(OriginAirportID = AirportID))
WDmFD<-rxMerge (inData1=dataWSdesport,inData2= dataFD,type="inner",matchVars=c("OriginAirportID"))


```

```{r SiB-Workshop - Day 1 - Example 1 – Basic Histogram}
library(shiny)
ui<- fluidPage(
titlePanel("Workshop - Example 1 – Basic Histogram"),
sidebarLayout(
sidebarPanel(
numericInput(inputId="n", label="Number of observations", value=1000)
),
mainPanel(plotOutput("plot"))
)
)
server<-function(input, output) {
data <- reactive({
x <- rnorm(input$n)
x
})
output$plot <- renderPlot({
hist(data(), 50,main="", xlab="x")
})
}
shinyApp(ui,server)
```


```{r SiBday1}
x <- runif(50); y <- 2*x + 0.5 + rnorm(50)/5
groups <- ifelse( y<1, 1, 2 )
par(las=1)
plot(x, y, col=c("red", "blue")[groups], pch=c(1,2)[groups])
title("Scatterplot of x vs y")
abline( lm( y ~ x) )
abline( h=1, lty=2 )
legend( x="topleft", legend=c("Group 1", "Group 2"),
fill=c("red","blue") )
text( 0,2, paste("Correlation = ", round(cor(x,y),2)), pos=4)
rug(x)
```

```{r SiBgoogleVis}
install.packages("googleVis")
library(googleVis)
data_survival<-read.csv("~/Documents/survival.csv", header=TRUE)
MCH<-gvisMotionChart(data,idvar = "",timevar = "",xvar=" ",yvar="",colorvar="",sizevar="",options = list())
MCH<-gvisMotionChart(data,idvar = "",timevar = "",xvar=" ",yvar="",colorvar="",sizevar="",options = list())
plot(gvisTable(data_survival))

df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), 
              val2=c(23,12,32))
```

```{r SiB}
library(googleVis)
data_map<-read.csv( "~/Documents/map.csv", header=TRUE,sep= "\t")
map<-gvisGeoChart(data_map,locationvar="Country",colorvar="mortality")
plot(map)
```
```{r SiBdataTable}
#excel limit by 60K rows
#R by 2GB lines*columns
memory.limit()
data("esoph")
object.size(esoph)
install.packages("SASxport")
library(SASxport)
brfss<-read.xport("~/LLCP2013.XPT")
object.size(brfss)
summary(brfss)
install.packages("data.table")
library(data.table)
brfss_dt<-data.table(brfss)
object.size(brfss_dt)
summary(brfss_dt)
brfss_dt<-data.table("~/LLCP2013.XPT")
brfss_dt$V1
object.size(brfss_dt)
summary(brfss_dt)
brfss_sample<-sample(t(brfss),50)
install.packages("oddsratio")
library(oddsratio)
library(epitools) 
oddsratio(as.factor(brfss$X.HCVU651),as.factor(brfss$X.RFCHOL))
rows_to_select <- sample(1:nrow(brfss), 500, replace=F) 
brfss_sample <- brfss[rows_to_select,]
oddsratio(as.factor(brfss_sample$X.HCVU651),as.factor(brfss_sample$X.RFCHOL))

```

```{r SiBbenchmark}
df <- data.frame(group=factor(sample(c("g1","g2"), 10, replace=TRUE)),mortality=runif(10))


library(microbenchmark)
library(ggplot2)
#install.packages("dplyr")
library(plyr)
library(dplyr)
library(reshape2)
library(data.table)
#install.packages('tidyverse')
library(tidyverse)

dt<- data.table(df)
setkey(dt, mortality)

setDT(df)[ , .(mean_mortality =mean(mortality)), by = group]


splitmean <- function(df) {
s <- split( df, df$group)
sapply( s, function(x)
mean(x$mortality) )
}
splitmean(df)


m1 <- microbenchmark(
by( df$mortality, df$group, mean),
aggregate(mortality~ group, df, mean ),
splitmean(df),
ddply( df, .(group), function(x) mean(x$mortality) ),
dcast( melt(df), variable ~ group, mean),
#dt[, mean(mortality), by = group],
summarize( group_by(df, group), m = mean(mortality) ),
summarize( group_by(dt, group), m = mean(mortality) )
)
print(m1, signif = 3)
autoplot(m1)
```


```{r SiBpack}
install.packages("epitools")
install.packages("taRifx")
install.packages("data.table")
install.packages("reshape")
install.packages("dplyr")
install.packages("plyr")
#install.packages("utils")
install.packages("microbenchmark")

install.packages("ggplot2")

install.packages("ggfortify")
```

```{r peptideDistribChk}
lPep=100
nPep=10000
nTot=lPep*nPep
data<-runif(nTot)
#hist(sample(data,100000,replace=T),breaks=nPep)
sPep=rnorm(lPep)
n=1
i=1
while(n<=lPep){
  print(n)
  print(i)
  sPep[c(seq(n,n+i-1))]=sample(data,i,replace=T)
  n=n+i
  i=i+1
}
hist(sPep,breaks=lPep)
summary(sPep)
```

```{r setup}
library(r2d3)
bars <- c(10, 20, 30)
```

```{r VennDetail}
#https://bioconductor.org/packages/release/bioc/vignettes/VennDetail/inst/doc/VennDetail.html
#install.packages('BiocManager')
BiocManager::install("VennDetail")
library(VennDetail)
data(T2DM)
ven <- venndetail(list(Cortex = T2DM$Cortex$Entrez, SCN = T2DM$SCN$Entrez,Glom = T2DM$Glom$Entrez))
plot(ven, type = "vennpie")
```

```{r proBatch}
#https://github.com/symbioticMe/proBatch
#needs R version 3.6 
bioc_deps <- c("GO.db", "impute", "preprocessCore", "pvca","sva" )
cran_deps <- c("corrplot", "data.table", "ggplot2", "ggfortify","lazyeval", "pheatmap", "reshape2", "rlang", 
               "tibble", "dplyr", "tidyr", "wesanderson","WGCNA") 

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(bioc_deps) 
install.packages(cran_deps)

install.packages("devtools")
install.packages("processx")
install.packages("dplyr")
install.packages("fs")
library(devtools)
install_github("symbioticMe/proBatch", build_vignettes = TRUE)

bioc_deps <- c("GO.db", "impute", "preprocessCore", "pvca","sva" ) 
cran_deps <- c("corrplot", "data.table", "ggplot2", "ggfortify","lazyeval", "pheatmap", "reshape2", "rlang" "tibble", "dplyr", "tidyr" "wesanderson","WGCNA")
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager") BiocManager::install(bioc_deps) install.packages(cran_deps)
BiocManager::install("proBatch")
#install.packages("devtools") devtools::install_github("symbioticMe/proBatch", build_vignettes = TRUE)
require(dplyr) require(tibble) require(ggplot2)
```

```{r PrInCE}
#https://bioconductor.org/packages/release/bioc/vignettes/PrInCE/inst/doc/intro-to-prince.html
BiocManager::install("PrInCE")
library(PrInCE)
data(scott)
PrInCE(scott, gold_standard)

library(r2d3)
bars <- c(10, 20, 30)
```

```{r PoTRA}
#https://bioconductor.org/packages/release/bioc/vignettes/PoTRA/inst/doc/PoTRA.html
BiocManager::install("PoTRA")
library(PoTRA) 
library(repmis) 
options(warn=-1) 
library(BiocGenerics) 
library(graph) 
library(graphite) 
library(igraph) 
source_data("https://github.com/GenomicsPrograms/example_data/raw/master/PoTRA-vignette.RData") 
humanKEGG <- pathways("hsapiens", "kegg") 
Pathway.database = humanKEGG 
results.KEGG <- PoTRA.corN(mydata=mydata, genelist=genelist, Num.sample.normal=113, Num.sample.case=113, Pathway.database=Pathway.database[1:15], PR.quantile=PR.quantile)
names(results.KEGG)
head(results.KEGG$Fishertest.p.value)

```
```{r PCAtools}
#https://bioconductor.org/packages/release/bioc/vignettes/PCAtools/inst/doc/PCAtools.html
devtools::install_github('kevinblighe/PCAtools')
library(PCAtools)
  p <- pca(x, metadata = metadata, removeVar = 0.1)
```

```{r pathwayPCA}
#https://github.com/gabrielodom/pathwayPCA
#install.packages("BiocManager")
BiocManager::install("pathwayPCA")

```

```{r animalcules}
#https://bioconductor.org/packages/release/bioc/vignettes/animalcules/inst/doc/animalcules.html
BiocManager::install("animalcules")
library(animalcules)

```

```{r AMARETTO}
#https://bioconductor.org/packages/release/bioc/vignettes/AMARETTO/inst/doc/amaretto.pdf
#https://github.com/gevaertlab/AMARETTO
BiocManager::install("AMARETTO")

```

```{r VCFArray}
#https://bioconductor.org/packages/release/bioc/vignettes/VCFArray/inst/doc/VCFArray.html
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
#The development version is also available to download from Github.
install.packages("remotes")
BiocManager::install("Bioconductor/VCFArray")
library(VCFArray)
```


```{r survtype}
#https://bioconductor.org/packages/release/bioc/vignettes/survtype/inst/doc/survtype.html
BiocManager::install("survtype")
library(survtype)
data(lung)
lung.survtype <- Surv.survtype(lung, time = "time", status = "status")
plot.survtype(lung.survtype, pval = TRUE)
```

```{r SubCellBarCode}
#https://bioconductor.org/packages/release/bioc/vignettes/SubCellBarCode/inst/doc/SubCellBarCode.html
BiocManager::install("SubCellBarCode")
library(SubCellBarCode)
head(markerProteins)
```

```{r StructuralVariantAnnotation}
#https://bioconductor.org/packages/release/bioc/vignettes/StructuralVariantAnnotation/inst/doc/vignettes.html
BiocManager::install("StructuralVariantAnnotation")
suppressPackageStartupMessages(require(StructuralVariantAnnotation))
suppressPackageStartupMessages(require(VariantAnnotation))
vcf.file <- system.file("extdata", "gridss.vcf", package = "StructuralVariantAnnotation")
vcf <- VariantAnnotation::readVcf(vcf.file, "hg19")
gr <- breakpointRanges(vcf)
partner(gr)

```

```{r SMAD}
#https://bioconductor.org/packages/release/bioc/vignettes/SMAD/inst/doc/quickstart.html
#BiocManager::install("SMAD")
library(SMAD)
data("TestDatInput")
head(TestDatInput)

scoreCompPASS <- CompPASS(TestDatInput)
head(scoreCompPASS)

scoreHG <- HG(TestDatInput)
head(scoreHG)

```

```{r RepViz}
#https://bioconductor.org/packages/release/bioc/vignettes/RepViz/inst/doc/RepViz.html
BiocManager::install("RepViz")
file.copy(from = list.files(system.file("extdata", package = "RepViz"), full.names = TRUE),to = tempdir())
setwd(tempdir())
region <- GRanges("chr12:110938000-110940000")
```

```{r qckitfastq}
#https://bioconductor.org/packages/release/bioc/vignettes/qckitfastq/inst/doc/vignette-qckitfastq.pdf
#BiocManager::install("qckitfastq")
library(qckitfastq)
infile <- system.file("extdata", "10^5_reads_test.fq.gz", package = "qckitfastq") fseq <- seqTools::fastqq(infile)
read_len <- read_length(fseq) 
kable(head(read_len)) %>% kable_styling()
```

```{r projectR}
#https://bioconductor.org/packages/release/bioc/vignettes/projectR/inst/doc/projectR.pdf
BiocManager::install("projectR")
library(projectR) 
projectR(data, loadings, dataNames=NULL, loadingsNames=NULL, NP = NULL, full = false)
```

```{r NLME}
library(nlme)
data(Loblolly)
fm1 <- nlsList(SSasymp, Loblolly)
fm1
fm2 <- nlme(fm1, random = Asym ~ 1)
fm2
q()
### Helical Valley Function
### Page 362 Dennis + Schnabel

require(stats); require(graphics)

theta <- function(x1,x2) (atan(x2/x1) + (if(x1 <= 0) pi else 0))/ (2*pi)
## but this is easier :
theta <- function(x1,x2) atan2(x2, x1)/(2*pi)

f <- function(x) {
    f1 <- 10*(x[3] - 10*theta(x[1],x[2]))
    f2 <- 10*(sqrt(x[1]^2+x[2]^2)-1)
    f3 <- x[3]
    return(f1^2+f2^2+f3^2)
}

## explore surface {at x3 = 0}
x <- seq(-1, 2, length.out=50)
y <- seq(-1, 1, length.out=50)
z <- apply(as.matrix(expand.grid(x, y)), 1, function(x) f(c(x, 0)))
contour(x, y, matrix(log10(z), 50, 50))
str(nlm.f <- nlm(f, c(-1,0,0), hessian = TRUE))
points(rbind(nlm.f$estim[1:2]), col = "red", pch = 20)

### the Rosenbrock banana valley function

fR <- function(x)
{
    x1 <- x[1]; x2 <- x[2]
    100*(x2 - x1*x1)^2 + (1-x1)^2
}

## explore surface
fx <- function(x)
{   ## `vectorized' version of fR()
    x1 <- x[,1]; x2 <- x[,2]
    100*(x2 - x1*x1)^2 + (1-x1)^2
}
x <- seq(-2, 2, length.out=100)
y <- seq(-0.5, 1.5, length.out=100)
z <- fx(expand.grid(x, y))
op <- par(mfrow = c(2,1), mar = 0.1 + c(3,3,0,0))
contour(x, y, matrix(log10(z), length(x)))

str(nlm.f2 <- nlm(fR, c(-1.2, 1), hessian = TRUE))
points(rbind(nlm.f2$estim[1:2]), col = "red", pch = 20)

## Zoom in :
rect(0.9, 0.9, 1.1, 1.1, border = "orange", lwd = 2)
x <- y <- seq(0.9, 1.1, length.out=100)
z <- fx(expand.grid(x, y))
contour(x, y, matrix(log10(z), length(x)))
mtext("zoomed in");box(col = "orange")
points(rbind(nlm.f2$estim[1:2]), col = "red", pch = 20)
par(op)


fg <- function(x)
{
    gr <- function(x1, x2) {
        c(-400*x1*(x2 - x1*x1)-2*(1-x1), 200*(x2 - x1*x1))
    }
    x1 <- x[1]; x2 <- x[2]
    res<- 100*(x2 - x1*x1)^2 + (1-x1)^2
    attr(res, "gradient") <- gr(x1, x2)
    return(res)
}

nlm(fg, c(-1.2, 1), hessian = TRUE)

## or use deriv to find the derivatives

fd <- deriv(~ 100*(x2 - x1*x1)^2 + (1-x1)^2, c("x1", "x2"))
fdd <- function(x1, x2) {}
body(fdd) <- fd
nlm(function(x) fdd(x[1], x[2]), c(-1.2,1), hessian = TRUE)


fgh <- function(x)
{
    gr <- function(x1, x2)
        c(-400*x1*(x2 - x1*x1) - 2*(1-x1), 200*(x2 - x1*x1))
    h <- function(x1, x2) {
        a11 <- 2 - 400*x2 + 1200*x1*x1
        a21 <- -400*x1
        matrix(c(a11, a21, a21, 200), 2, 2)
    }
    x1 <- x[1]; x2 <- x[2]
    res<- 100*(x2 - x1*x1)^2 + (1-x1)^2
    attr(res, "gradient") <- gr(x1, x2)
    attr(res, "hessian") <- h(x1, x2)
    return(res)
}

nlm(fgh, c(-1.2,1), hessian = TRUE)
```


```{d3 data=bars, options=list(color = 'orange')}
svg.selectAll('rect')
  .data(data)
  .enter()
    .append('rect')
      .attr('width', function(d) { return d * 10; })
      .attr('height', '20px')
      .attr('y', function(d, i) { return i * 22; })
      .attr('fill', options.color);
```

```{r data, echo = FALSE}
scriptD <- 'C:\\Users\\animeshs\\Desktop\\scripts\\'
inpD <-'L:\\promec\\Elite\\LARS\\2014\\desember\\christiano\\'
data <- read.delim(paste0(inpD,"proteinGroups.txt"),row.names=1,sep="\t",header = T)
summary(data)
decoyPrefix="REV"
contaminantPrefix="CON"
dataC="LFQ.intensity."
library(nlme)

options(digits=4) # avoid rounding differences

Ovary[c(1,272), 2] <- NA
fm1 <- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), Ovary,
           correlation = corAR1(form = ~ 1 | Mare), na.action=na.exclude)
fitted(fm1)
residuals(fm1)
summary(fm1)

Orthodont[100:102, 2] <- NA
fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1,
           na.action=na.exclude)
fitted(fm2, 0:1)
fitted(fm2)
residuals(fm2, 0:1)
round(residuals(fm2), 2)
summary(fm2)

Soybean[1:5, "Time"] <- NA
fm3 <- gnls(weight ~ SSlogis(Time, Asym, xmid, scal), Soybean,
            weights = varPower(), na.action=na.exclude)
fitted(fm3)
residuals(fm3)
summary(fm3)
```

```{r ROTS, echo = FALSE}
#install.packages("BiocManager")
#BiocManager::install("ROTS", version = "3.8")
BiocManager::install("ROTS")
dataNormImpCom[is.na(dataNormImpCom)]=5
summary(dataNormImpCom)
library(ROTS)
data(upsSpikeIn)
input = upsSpikeIn
groups = c(rep(0,3), rep(1,3))
groups
results = ROTS(data = input, groups = groups , B = 100 , K = 500 , seed = 1234)
names(results) 
summary(results, fdr = 0.05)
plot(results, fdr = 0.2, type = "volcano")
#plot(results, fdr = 0.05, type = "heatmap")
```

```{r reticulate, echo = FALSE}
#https://rviews.rstudio.com/2019/03/18/the-reticulate-package-solves-the-hardest-problem-in-data-science-people/
install.packages('mlbench') #provides the data set
library(mlbench) #provides the data set
data("BreastCancer")
#convert to numeric for models and remove na values for this example
install.packages('xgboost')
library(xgboost)

model_set <- sapply(BreastCancer[complete.cases(BreastCancer),-1], as.numeric) 

#format target variable as 0, 1 instead of 1,2
model_set[,10]<-model_set[,10]-1 
  
#Split into test and train sets
indices <- sample(1:nrow(model_set), size = 0.7 * nrow(model_set))

#Target variables
target<-unlist(model_set[indices,10])
test_target<-unlist(model_set[-indices,10])

#create unscaled data set for boosted tree models
unscale_train<-as.matrix(model_set[indices,-10])
unscale_test<-as.matrix(model_set[-indices,-10 ])

#create normalized data set for neural network
mean <- apply(model_set[indices,-10], 2, mean)
std <- apply(model_set[indices,-10], 2, sd)

train <- scale(model_set[indices,-10], center = mean, scale = std)
test <- scale(model_set[-indices,-10], center = mean, scale = std)

boost_model<-xgboost(data = unscale_train,label=target,booster="gbtree", nfold = 2,nrounds = 25, verbose = FALSE, objective = "binary:logistic", eval_metric = "auc", nthread = 4)

install.packages('dplyr')
library(dplyr)

install.packages('keras')
library(keras)
y_target<-to_categorical(target,2)

tf_nn <- keras_model_sequential() %>%
  layer_dense(units = 12,
              activation = 'relu',
              input_shape = dim(train)[[2]]) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 12,
              activation = 'relu')%>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2,
              activation = 'softmax')


tf_nn %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

history<-tf_nn %>% fit(
  x=train,
  y=y_target,
  epochs = 7,
  batch_size = 12
)

```

```{r stan, echo = FALSE}
library('rstan')
stan(paste0(scriptD,'school8.stan'))
```


```{r tmp, echo = FALSE}
frac $\frac{1}{n}$ 
memory.size(TRUE)
memory.limit()
rm(list=ls())
```

```{r python, echo = FALSE}
#install.packages('reticulate')
#install.packages("reticulate")
#devtools::install_github("rstudio/reticulate")
#library('reticulate')
#Sys.which("python")
devtools::install_github("rstudio/reticulate")
```

```{r d3, echo = FALSE}
#install.packages('devtools')
#devtools::install_github("rstudio/r2d3")
library(r2d3)
r2d3(data=c(0.3, 0.6, 0.8, 0.95, 0.40, 0.20), script = "barchart.js")
a=c(c(1,2) c(2,1))
dim(a)<-c(2,2)
(a)*t(a)
cov(a)
```



```{r shiny, echo = FALSE}
#https://rmarkdown.rstudio.com/authoring_shiny.html
#install.packages('shiny')
library(shiny)
dN_shiny <- dataNorm
#ctrl-shift-K
#rmarkdown::run
#shiny::renderUI
```

```{r slider, echo=FALSE}

inputPanel(
  sliderInput("bins", "#bins:", min = 1, max = 100, value = 30),
  textInput("dens", "Density:", "auto",value = 0.6), 
  selectInput("expression", label = "Sample:",choices = colnames(dN_shiny), selected = colnames(dN_shiny)[1]))
renderText({paste("Inputs:", input$bins,input$dens,input$expression)})
```

```{r plot, echo = FALSE}
renderPlot({
  hist(dN_shiny[,as.numeric(which(colnames(dN_shiny) == input$expression))],breaks = as.numeric(input$bins),probability = TRUE,col="orange")
  lines(density(dN_shiny[,as.numeric(which(colnames(dN_shiny) == input$expression))], adjust = as.numeric(input$dens)),col="blue")
  })
  
```

```{r UWOT, echo = FALSE}
#devtools::load_all()
#install.packages('devtools')
#library('devtools')
#install_github("jlmelville/uwot")
library()
iris_umap <- umap(iris, n_neighbors = 50, learning_rate = 0.5, init = "random")

# Load mnist from somewhere, e.g.
# devtools::install_github("jlmelville/snedata")
# mnist <- snedata::download_mnist()
mnist_umap <- umap(mnist, n_neighbors = 15, min_dist = 0.001, verbose = TRUE)

# Use a specific number of threads
mnist_umap <- umap(mnist, n_neighbors = 15, min_dist = 0.001, verbose = TRUE, n_threads = 8)

# Use a different metric
mnist_umap_cosine <- umap(mnist ,n_neighbors = 15, metric = "cosine", min_dist = 0.001, verbose = TRUE, n_threads = 8)

# Supervised dimension reduction
mnist_umap_s <- umap(mnist, n_neighbors = 15, min_dist = 0.001, verbose = TRUE, n_threads = 8, 
                     y = mnist$Label, target_weight = 0.5)
                    
# Add new points to an existing embedding
mnist_train <- head(mnist, 60000)
mnist_test <- tail(mnist, 10000)

# You must set ret_model = TRUE to return extra data we need
# coordinates are in mnist_train_umap$embedding
mnist_train_umap <- umap(mnist_train, verbose = TRUE, ret_model = TRUE)
mnist_test_umap <- umap_transform(mnist_test, mnist_train_umap, verbose = TRUE)

# Save the nearest neighbor data
mnist_nn <- umap(mnist, ret_nn = TRUE)
# coordinates are now in mnist_nn$embedding

# Re-use the nearest neighor data and save a lot of time
mnist_nn_spca <- umap(mnist, nn_method = mnist_nn$nn, init = spca)

# No problem to have ret_nn = TRUE and ret_model = TRUE at the same time

# Calculate Petal and Sepal neighbors separately (uses intersection of the resulting sets):
iris_umap <- umap(iris, metric = list("euclidean" = c("Sepal.Length", "Sepal.Width"),
                                      "euclidean" = c("Petal.Length", "Petal.Width")))
# Can also use individual factor columns
iris_umap <- umap(iris, metric = list("euclidean" = c("Sepal.Length", "Sepal.Width"),
                                      "euclidean" = c("Petal.Length", "Petal.Width"),
                                      "categorical" = "Species"))
                                      
# MNIST with PCA reduction to 50 dimensions can speed up calculation without
# affecting results much
mnist_umap <- umap(mnist, pca = 50)

```


```{r clusterProfiler, echo = FALSE}
#BiocManager::install("STRINGdb")
library(STRINGdb)
string_db <- STRINGdb$new( version="11", species=9606,score_threshold=0, input_directory="" )
UniprotStrings<-as.data.frame(Uniprot)
example1_mapped <- string_db$map( UniprotStrings, "Uniprot", removeUnmappedRows = TRUE )
hits<-example1_mapped$STRING_id
enrichmentGO <- string_db$get_enrichment( hits, category = "Process", methodMT = "fdr", iea = TRUE )
enrichmentKEGG <- string_db$get_enrichment( hits, category = "KEGG", methodMT = "fdr", iea = TRUE )
head(enrichmentGO, n=7)
head(enrichmentKEGG, n=7)
```



```{r clusterProfiler, echo = FALSE}
install.packages('BiocManager')
BiocManager::install('goseq')
library(goseq)
supportedOrganisms() 
#https://bioconductor.org/packages/release/bioc/vignettes/clusterProfiler/inst/doc/clusterProfiler.html
#source("https://bioconductor.org/biocLite.R")
## biocLite("BiocUpgrade") ## you may need this
#biocLite("clusterProfiler")
#biocLite("org.Hs.eg.db")
library("org.Hs.eg.db")
#install.packages("colorspace")
#devtools::install_github('cran/colorspace')
library("clusterProfiler")
Uniprot=data[data$`MCCAR Biol Rep 25 WSRT``NB4 Biol Rep 22 WSRT`<0.5,1]
dataSub=subset(data,`NB4 Biol Rep 22 WSRT`<0.3 & `MCCAR Biol Rep 25 WSRT`<0.3)
Uniprot<-sapply(strsplit(dataSub$`T: Majority protein IDs`,";"), `[`, 1)
enrichGO(gene=Uniprot,OrgDb=org.Hs.eg.db,keyType= 'UNIPROT',ont= "CC",pAdjustMethod = "BH",pvalueCutoff  = 0.01,qvalueCutoff  = 0.05)
UniprotEG<-bitr(Uniprot, fromType="UNIPROT", toType="ENTREZID", OrgDb="org.Hs.eg.db")
UniprotKEGG<-bitr(Uniprot, fromType="UNIPROT", toType="KEGG", OrgDb="org.Hs.eg.db")
UniprotKEGG<-bitr_kegg(Uniprot, fromType='uniprot', toType='kegg', organism='hsa')
kk <- enrichKEGG(gene=UniprotEG$ENTREZID)
kk@result[["Description"]]
browseKEGG(kk,kk@result[["ID"]][1])
```


```{r installs}
install.packages(c("matrixStats", "Hmisc", "splines", "foreach", "doParallel", "fastcluster", "dynamicTreeCut", "survival"))
source("http://bioconductor.org/biocLite.R") 
biocLite(c("GO.db", "preprocessCore", "impute"))
orgCodes = c("Hs", "Mm", "Rn", "Pf", "Sc", "Dm", "Bt", "Ce", "Cf", "Dr", "Gg"); 
orgExtensions = c(rep(".eg", 4), ".sgd", rep(".eg", 6)); 
packageNames = paste("org.", orgCodes, orgExtensions, ".db", sep=""); 

biocLite(c("GO.db", "KEGG.db", "topGO", packageNames, "hgu133a.db", "hgu95av2.db", "annotate", "hgu133plus2.db", "SNPlocs.Hsapiens.dbSNP.20100427", "minet", "OrderedList"))
install.packages("BiocManager") 
BiocManager::install("WGCNA") 
library(WGCNA);
allowWGCNAThreads()
```


```{r data}
femData <- read.csv("https://raw.githubusercontent.com/iamciera/10wgcna/master/examples/LiverFemale3600.csv")
names(femData)

#This is just melting the data without the first 8 columns or something
datExpr0 <- as.data.frame(t(femData[, -c(1:8)]))
names(datExpr0) = femData$substanceBXH
rownames(datExpr0) = names(femData)[-c(1:8)]

head(datExpr0[,1:8])

datExpr0 = as.data.frame(t(femData[, -c(1:8)]));
gsg = goodSamplesGenes(datExpr0, verbose = 3)
gsg$allOK
install.packages('flashClust')
library(flashClust)
sampleTree = flashClust(dist(datExpr0), method = "average");
plot(sampleTree, main = "Sample clustering to detect outliers", sub="", xlab="", cex.lab = 1.5,
     cex.axis = 1.5, cex.main = 2)

net = blockwiseModules(datExpr0, power = 6,
                       TOMType = "unsigned", minModuleSize = 30,
                       reassignThreshold = 0, mergeCutHeight = 0.25,
                       numericLabels = TRUE, pamRespectsDendro = FALSE,
                       saveTOMs = TRUE,
                       saveTOMFileBase = "femaleMouseTOM", 
                       verbose = 3)
#https://jolars.github.io/eulerr/articles/venn-diagrams.html
install.packages('eulerr')
library(eulerr)
s4 <- list(a = c(1, 2, 3),
           b = c(1, 2),
           c = c(1, 4),
           e = c(5))
plot(venn(s4))
plot(euler(s4, shape = "ellipse"), quantities = TRUE)

#https://github.com/jolars/eulerr.co/blob/master/server.R

library(ggplot2)

#heatmap(data$Difference)
p <- ggplot(data,aes(Difference,X.Log.P.value.))
p<-p + geom_tile(aes(fill=X.Log.P.value.)) + scale_fill_gradient(low="white", high="darkblue") + xlab("") + ylab("")
f=paste(file,proc.time()[3],".jpg")
ggsave(filename=f, plot=p)
print(p)
data <- read.delim("Y:/felles/Voin/===Methodology paper===/Supplementary table Ttest BothSided Sample.txt",row.names=22,sep="\t",header = T)
data <- read.delim("Supplementary table Ttest BothSided",row.names=22,sep="\t",header = T)
summary(data)
```

```{r xplot}

library('ggplot2')
data[1,]
Significance=data$X.Log.P.value.>-log10(0.05)&abs(data$Difference)>log2(1.5)
sum(Significance)
dsub <- subset(data, data$X.Log.P.value.==max(data$X.Log.P.value.)|data$Difference==max(abs(data$Difference)))
dsub$Gene.names
plot(data$Difference,data$X.Log.P.value.)
qplot(Difference,X.Log.P.value.,data=data,color=X.Log.P.value.>-log10(0.05)&abs(Difference)>log2(1.5))
g = ggplot(data,aes(Difference,X.Log.P.value.))
gps<-g + geom_point(aes(color=Significance)) + theme_bw(base_size=10) + geom_text(data=dsub,aes(label=Gene.names),hjust=0, vjust=0) + xlab("Log2 Fold Change")  + ylab("-Log10 P-value") + ggtitle("Differentially expressed proteins") + scale_size_area()
p=paste(file,proc.time()[3],".jpg")
ggsave(filename=p, plot=gps)
```



```{r, echo=FALSE}
plot(log2(data$A549.Cis.1/100),log2(data$A549.Cis.2/100),col="#FF00FF",cex=0.5,pch=16,lty=1)
```

```{r sign-test}

set.seed(2016)
bio <- 3
prot <- nrow(data)
wilcox.test(data)
dataa549cis = t(data[,grepl( "^A549.Cis",names(data))])
dataa549cis[is.nan(dataa549cis)] <- NA
dataa549cis = t(c(-1,2,3,4,50))
results <- apply(dataa549cis, 1, function(dataa549cis) {
  wilcox.test(dataa549cis)$p.value})
results


results <- apply(dat, 1, function(dat) {
  wilcox.test(x = dat[1:x])$p.value})


results <- apply(data, 1, function(data) {
  wilcox.test(x = data[,grepl( "^A549.Cis",names(data))])$p.value})

results <- apply(data, 1, function(data) {
  wilcox.test(x = log2(data[,grepl( "^A549.Cis",names(data))])$p.value}))
results <- apply(data, 1, function(data) {
  wilcox.test(x=log2(data$A549.Cis.1/100))$p.value})
cbind(data, pvals = results)

hist(p.adjust(results,method="BH"))
hist(p.adjust(results,method="BH"))
hist(p.adjust(results,method="holm"))
?p.adjust
hist(results)
min(results)
data=read.delim("C:/Users//animeshs/Google Drive/wilcox_sgn_rnk_wiki.txt",row.names=1)
wilcox.test(data$x2,data$x1,paired = TRUE, alternative = "less")
wilcox.test(data$x2,data$x1,paired = TRUE, alternative = "greater")
wilcox.test(data$x2,data$x1,paired = TRUE)
wilcox.test(c(1,1,1,1,1))
0.5^4
wilcox.test((data$x2-data$x1))$pvalue
binom.test(3,3)
sum(data$sgn>0, na.rm=TRUE)
binom.test(1,3)
t.test(c(data$x1,data$x2),c(data$abs,data$abs))
t.test(extra ~ group, data = sleep)
wilcox.test(extra ~ group, data = sleep)


x <- 10; y <- 10; g <- 1000
mean(c(1,2,3))
set.seed(1969)
dat <- matrix(rnorm((x + y) * g), ncol = x + y)

results <- apply(dat, 1, function(dat) {
  wilcox.test(x = dat[1:x])$p.value})

cbind(dat, pvals = results)
hist(results,col="orange")
hist(p.adjust(results,method="BH"))

#write.csv2("Y:/felles/Voin/===Methodology paper===/Supplementary table WCSR_test.txt",results)
install.packages("rlm")
library(rlm)
?rlm
qqnorm(c(1,2,10,100))
?qqplot


install.packages("outliers")
library(outliers)
grubbs.test(c(10,20, 200))



```





```{r MSstat}
source("http://bioconductor.org/biocLite.R")
biocLite("MSstats")
library("MSstats")


```

```{r interactive}
#install.packages("shiny")
library(shiny)
runExample("01_hello")
```

```{r google}
install.packages("RGoogleAnalytics")
library("RGoogleAnalytics")
```

```{r googleVis}
#install.packages("googleVis")
#suppressPackageStartupMessages(library(googleVis))
#http://rpubs.com/gallery/googleVis
T <- gvisTable(Exports, options = list(width = 200, height = 280))
G <- gvisGeoChart(Exports, locationvar = "Country", colorvar = "Profit", 
    options = list(width = 360, height = 280, dataMode = "regions"))
TG <- gvisMerge(T, G, horizontal = TRUE, tableOptions = "bgcolor=\"#CCCCCC\" cellspacing=10")

print(TG, "chart")
```

**check**
```{r Hurricane Andrew (1992) storm track with Google Maps}
AndrewMap <- gvisMap(Andrew, "LatLong", "Tip", options = list(showTip = TRUE, 
    showLine = TRUE, enableScrollWheel = TRUE, mapType = "hybrid", useMapTypeControl = TRUE))

print(AndrewMap, "chart")
```

**check**
```{r fig.width=7, fig.height=6}
## Table with embedded links
PopTable <- gvisTable(Population, options = list(width = 600, height = 300, 
    page = "enable"))

print(PopTable, "chart")
```


```{r data}
data <- read.delim("Y:/felles/PROTEOMICS and XRAY/Results/Kristian/a-GEIR/Figurer/Bakgrunnsmateriale/Table 1 og figur 3/20160417_tabell fra Perseus. Slettet RPS10p5.txt")
data <- read.delim("L:/Results//TObermann/data.txt")
summary(data)
d1<-read.delim('Y:/felles/PROTEOMICS and XRAY/Ani/misccb/d1.txt')
d2<-read.delim('Y:/felles/PROTEOMICS and XRAY/Ani/misccb/d2.txt')
summary(d1)
summary(d2)
d<-merge(d1,d2,by="ID",all=T)
summary(d)
library(plyr)
install.packages('plyr')
d<-merge(d1,d2)
d3<-d1
summary(d)
d<-merge(d1,d3,by="ID",all=T)
install.packages("profvis")

```


```{r ColumnSel}
IP0h<-data[, grep("^X.IP..0.h.....IP..PBS.....$", colnames(data))]
summary(IP0h)
heatmap(na.omit(as.matrix(IP0h)))
mapply(t.test,IP0h[,])
```

```{r model}
library('nlm')
plot(log(Fnorm) ~ log(Concentration), data=data)
yp=(max(data$Fnorm)-data$Fnorm)/(max(data$Fnorm)-min(data$Fnorm))
yq=1-yp
glm.out = glm(cbind(yp,yq) ~ log(data$Concentration), family=binomial(logit))
lines(log(data$Concentration), glm.out$fitted, type="l", col="red")
summary(glm.out)
source("http://bioconductor.org/biocLite.R")
biocLite("clusterProfiler")
library("clusterProfiler")
data(gcSample)
x <- groupGO(gene=gcSample[[1]],organism="human",ont="CC",level=2,readable=TRUE)
head(summary(x))
x
id=read.table("ListOfTWLUniprotNIFEIDtsv.txt")
x <- groupGO(gene=(as.character(as.matrix(id))),organism="human",ont="CC",level=4,readable=TRUE)
head(summary(x))
x <- groupGO(gene=(as.character(as.matrix(id))),organism="human",ont="CC",level=4,readable=TRUE)
head(summary(x))
page(summary(x))
plot(x)
dline <- read.table("res_line.txt")
resdataap <- read.table("ap.txt")
resdataapip <- read.table("apip.txt")
resdatafl <- read.table("fl.txt")

conc=c(resdataap$ConcPm,resdataapip$ConcPm,resdatafl$ConcPm)
lethal=c(resdataap$Ap,resdataapip$Ap_Ip,resdatafl$Fl)



x <- seq(0, 2000, length=201)
yap <- (dline$AP[1]*x+dline$AP[2])
yapip <- (dline$AP_IP[1]*x+dline$AP_IP[2])
yfl <- (dline$FL[1]*x+dline$FL[2])


plot(conc,lethal)
lines(x, yap, type = "l", col="red")
lines(x, yapip, type = "l", col="green")
lines(x, yfl, type = "l", col="blue")


write.table((cbind(c(x),c(yap))), file = "ap_otp.txt", sep = "\t",col.names = FALSE, row.names = FALSE )


write.table((cbind(c(x),c(yapip))), file = "apip_otp.txt", sep = "\t",col.names = FALSE, row.names = FALSE )

write.table((cbind(c(x),c(yfl))), file = "fl_otp.txt", sep = "\t",col.names = FALSE, row.names = FALSE )

map <- read.table("maplot.txt")
matplot(map)

x <- seq(0, 2000, length=201)
yap <- (dline$AP[1]*x+dline$AP[2])
plot(x, yap, type = "l", col="red")


plot.new()
line(dline$AP[2],dline$AP[1])

lmfit <- lm(Ap~ConcPm, data=resdataap, na.action=na.omit)
summary(lmfit)
plot(resdataap$ConcPm,resdataap$Ap)
abline(coef(lmfit))

lmfit <- lm(Ap_Ip~ConcPm, data=resdataapip, na.action=na.omit)
summary(lmfit)
plot(resdataapip$ConcPm,resdataapip$Ap_Ip)
abline(coef(lmfit))

lmfit <- lm(Fl~ConcPm, data=resdatafl, na.action=na.omit)
summary(lmfit)
plot(resdatafl$ConcPm,resdatafl$Fl)
abline(coef(lmfit))




resdataap$ApT <- resdataap$ConcPm/resdataap$Ap
lmfit <- lm(Ap~ConcPm, data=resdataap, na.action=na.omit)
plot(resdataap$ConcPm, resdataap$ApT)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Ap~Bm*ConcPm/(Kd+ConcPm),data=resdataap, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resdataap$ConcPm, resdataap$Ap)
x <- seq(0, 2000, length=10)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(ConcPm=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")



lmfit <- lm(Fl~Conc, data=resdata, na.action=na.omit)
summary(lmfit)
plot(resdata$Conc,resdata$Fl)
abline(coef(lmfit))






resapc3$Pbtrans <- resapc3$Pf/resapc3$Pb
lmfit <- lm(Pbtrans~Pf, data=resapc3, na.action=na.omit)
plot(resapc3$Pf, resapc3$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resapc3, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resapc3$Pf, resapc3$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")



resfl$Pbtrans <- resfl$Pf/resfl$Pb
lmfit <- lm(Pbtrans~Pf, data=resfl, na.action=na.omit)
plot(resfl$Pf, resfl$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resfl, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resfl$Pf, resfl$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")




resflc3$Pbtrans <- resflc3$Pf/resflc3$Pb
lmfit <- lm(Pbtrans~Pf, data=resflc3, na.action=na.omit)
plot(resflc3$Pf, resflc3$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resflc3, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resflc3$Pf, resflc3$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")




resm347$Pbtrans <- resm347$Pf/resm347$Pb
lmfit <- lm(Pbtrans~Pf, data=resm347, na.action=na.omit)
plot(resm347$Pf, resm347$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resm347, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resm347$Pf, resm347$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")





resm356$Pbtrans <- resm356$Pf/resm356$Pb
lmfit <- lm(Pbtrans~Pf, data=resm356, na.action=na.omit)
plot(resm356$Pf, resm356$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resm356, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resm356$Pf, resm356$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")



resdm$Pbtrans <- resdm$Pf/resdm$Pb
lmfit <- lm(Pbtrans~Pf, data=resdm, na.action=na.omit)
plot(resdm$Pf, resdm$Pbtrans)
abline(coef(lmfit))
Bm <- 1/coef(lmfit)[2]
Kd <- Bm*coef(lmfit)[1]
Bm
Kd
nlsfit <- nls(Pb~Bm*Pf/(Kd+Pf),data=resdm, start=list(Kd=Kd, Bm=Bm))
summary(nlsfit)
plot(resdm$Pf, resdm$Pb)
x <- seq(0, 60, length=120)
y2 <- (coef(nlsfit)["Bm"]*x)/(coef(nlsfit)["Kd"]+x)
y2 <- predict(nlsfit,data.frame(conc=x))
lines(x, y2)
y1 <- (Bm*x)/(Kd+x)
lines(x, y1, lty="dotted", col="red")








t.test((resap$Pb/(resap$Pb+resap$Pf)),(resfl$Pb/(resfl$Pb+resfl$Pf)))
t.test((resap$Pb/(resap$Pb+resap$Pf)),(resm347$Pb/(resm347$Pb+resm347$Pf)))
t.test((resap$Pb/(resap$Pb+resap$Pf)),(resm356$Pb/(resm356$Pb+resm356$Pf)))
t.test((resap$Pb/(resap$Pb+resap$Pf)),(resdm$Pb/(resdm$Pb+resdm$Pf)))

t.test((resfl$Pb/(resfl$Pb+resfl$Pf)),(resm347$Pb/(resm347$Pb+resm347$Pf)))
t.test((resfl$Pb/(resfl$Pb+resfl$Pf)),(resm356$Pb/(resm356$Pb+resm356$Pf)))
t.test((resfl$Pb/(resfl$Pb+resfl$Pf)),(resdm$Pb/(resdm$Pb+resdm$Pf)))

t.test((resm347$Pb/(resm347$Pb+resm347$Pf)),(resm356$Pb/(resm356$Pb+resm356$Pf)))
t.test((resm347$Pb/(resm347$Pb+resm347$Pf)),(resdm$Pb/(resdm$Pb+resdm$Pf)))

t.test((resm356$Pb/(resm356$Pb+resm356$Pf)),(resdm$Pb/(resdm$Pb+resdm$Pf)))




names(lmfit)
summary(lmfit)
plot(lmfit)
class(lmfit)
coef(lmfit)



p = res1$WT
q = res1$WT_347
r = res1$WT_356
s = res1$WT_347_356
scores = data.frame(p,q,r,s)
boxplot(scores)
scores = stack(scores)
names(scores)
oneway.test(values ~ ind, data=scores, var.equal=T)
```


```{r VennDetail}
#https://bioconductor.org/packages/release/bioc/vignettes/VennDetail/inst/doc/VennDetail.html
#install.packages('BiocManager')
#BiocManager::install("VennDetail")
library(VennDetail)
#ven <- venndetail(list(dat=dataClean$Majority.protein.IDs,daf=df.prot$Majority.protein.IDs))
#plot(ven)
#head(dataNorm$LFQ.intensity.15+dataNorm$LFQ.intensity.12)
#ven <- venndetail(list(Red = dataNorm$LFQ.intensity.19+dataNorm$LFQ.intensity.26+dataNorm$LFQ.intensity.29, White = dataNorm$LFQ.intensity.15+dataNorm$LFQ.intensity.12+dataNorm$LFQ.intensity.30))
ven <- venndetail(list(Red = dataNormFilter$Red, White = dataNormFilter$White))
#ven <- venndetail(dataNorm)
plot(ven)
plot(ven, type = "vennpie")
```
